**[中文](./README_ZH.md) | [English](./README.md)** 

# ChatGPT在学术圈

随着自然语言处理（NLP）技术的发展，像ChatGPT这样的大型语言模型（LLMs）因其在科学文献写作中的潜力而受到了广泛关注。然而，人们也对在科学写作中使用大型语言模型的伦理影响和可靠性产生了疑虑。

**本目录旨在收集和展示不同出版商和会议对在科学文献写作中使用LLMs的政策，以帮助作者快速了解在科学文献写作中哪些关于使用LLMs的行为是不认可的。**

我们希望本目录能够促进关于LLMs在科学文献写作中的作用的开放透明的讨论，并促进这些技术以负责任和符合伦理道德的情况下使用。

*如果您有任何相关新闻要分享，请随时创建一个PR！*




## Policy at a Glance

| 出版商           |   最后更新   | 文本润色 | 文献检索 | 新思路 | ChatGPT作为作者 |
| ---------------- | :----------: | :------: | :------: | :----: | :-------------: |
| Nature Portfolio | Jan 24, 2023 |    ⚠️     |    ⚠️     |   ⚠️    |        ❌        |
| Science          | Jan 26, 2023 |    ❌     |    ❓     |   ❓    |        ❌        |
| arXiv preprint   | Jan 31, 2023 |    ⚠️     |    ❓     |   ❓    |        ❌        |
| Elsevier         |     n/a      |    ✅     |    ❓     |   ❓    |        ❌        |
| ICML 2023        |  Jan, 2023   |    ✅     |    ❓     |   ❓    |        ❌        |
| ACL 2023         |  Jan, 2023   |    ✅     |    ⚠️     |   ⚠️    |        ❌        |


✅ 可以使用 (有明确的官方规定)

❌ 不可使用 (有明确的官方规定)

⚠️ 谨慎使用 (没有明确的官方规定)

❓ 未知



## 出版商

### Nature

**[20 Feb 2023] [How Nature readers are using ChatGPT](https://www.nature.com/articles/d41586-023-00500-8)**

- 简述：80%的受访者使用过AI聊天机器人，其中57%表示他们将其用于“创意娱乐”。

**[17 Feb 2023] [How will AI change mathematics? Rise of chatbots highlights discussion](https://www.nature.com/articles/d41586-023-00487-2)**

- 简述：机器学习工具已经帮助数学家制定新理论并解决难题。但它们将进一步颠覆这个领域。

**[03 Feb 2023] [ChatGPT: Five priorities for research](https://www.nature.com/articles/d41586-023-00288-7)**

- 简述：对话型人工智能对科学来说是一个颠覆者。

**[24 Jan 2023] [Tools such as ChatGPT threaten transparent science; here are our ground rules for their use](https://www.nature.com/articles/d41586-023-00191-1)**

- 简述：Nature Portfolio已经制定了他们对LLM（如ChatGPT）使用的规则。1）LLM不能作为作者列出；2）使用LLM的情况应该清楚地描述其范围。

### Science (AAAS)

**[26 Jan 2023] [ChatGPT is fun, but not an author](https://www.science.org/doi/10.1126/science.adg7879)**

- 简述：在*Science*期刊上发表的论文中，不能使用来自人工智能、机器学习或类似算法工具生成的文本、图表或图片，除非获得编辑的明确许可。

### arXiv

**[31 Jan 2023] [arXiv policy for authors’ use of generative AI language tools](https://blog.arxiv.org/2023/01/31/arxiv-announces-new-policy-on-chatgpt-and-similar-tools/)**

- 简述：1）LLM不能作为作者列出；2）无论内容是如何生成的，作者都应该承担责任；3）在论文中报告是否使用了生成式AI来创建论文。

### Elsevier

**[Editorial Policy: The use of AI and AI-assisted Technologies in Scientific Writing](https://www.elsevier.com/about/policies/publishing-ethics)**

- 简述：Elsevier针对生成式AI和AI辅助技术在内容创作中的日益普及发布了一项政策。具体内容包括：1）作者应在手稿中披露使用AI和AI辅助技术，并仅使用它们来改善可读性和语言，而不是替代关键研究任务；2）作者最终对作品内容负责，并应仔细审查和编辑AI生成内容的结果；3）作者不应将AI和AI辅助技术列为作者或合著者，并应确保作品是原创的，不侵犯第三方权利。

## 会议

### ICML 2023 (International Conference on Machine Learning)

简述：

- ICML 2023的大型语言模型（LLM）政策禁止完全由LLMs生成的文本。这并不禁止作者使用LLMs进行编辑或润色作者编写的文本。
- LLM政策在很大程度上基于保守原则，旨在防范使用LLMs可能带来的潜在问题，包括抄袭。
- LLM政策适用于ICML 2023。随着我们更好地理解LLMs及其对科学出版的影响，我们预计未来的会议中这一政策可能会有所发展。

相关链接: https://icml.cc/Conferences/2023/llm-policy

### ACL 2023 (Annual Meeting of the Association for Computational Linguistics)

简述：

- 允许进行轻微编辑和语法修正，无需进行明确声明。

- 对于文献检索、提供新思路的情况，请谨慎使用并确保相关文献得到适当引用。
- ACL不鼓励作者完全依赖LLM来生成论文内容。
- 对于AI辅助编程，作者应明确展示其使用范围。

相关链接: https://2023.aclweb.org/blog/ACL-2023-policy/



## "反"ChatGPT 工具

为了识别文本是否来自AI辅助引擎（例如ChatGPT）或人类，业界已经发布了几种工具。这些工具可能有助于识别潜在的抄袭和学术不端行为。

**💡提示：根据个人使用经验，目前这些工具的识别结果并不完全可靠。**

**⚠️警告：请谨慎使用，注意这些工具的数据收集问题，特别是对于私人数据。**

### [GPTZero](https://gptzero.me)

简述：这是一个API，用于检测文本是否由人工智能生成。接受文件和文本输入，返回句子、段落和文档级别的概率。

特点：
- 至少需要250个字符
- 支持pdf、docx、txt格式
- 支持API请求
- 对输入文本进行详细评分

### [OpenAI AI Text Classifier](https://platform.openai.com/ai-text-classifier)

简述：这是一个OpenAI官方推出的AI文本分类器，用于区分AI编写和人类编写的文本。

博客：https://openai.com/blog/new-ai-classifier-for-indicating-ai-written-text/

特点：

- 需要至少1,000个字符，大约是150-250个单词。
- 有五个结果等级：非常不可能、不太可能、不确定、可能是、或者很可能是AI生成的。